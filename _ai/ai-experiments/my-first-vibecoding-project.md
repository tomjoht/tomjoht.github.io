---
title:  "My first vibecoding project (AI experiments)"
permalink: ai/my-first-vibecoding-project.html
course: "AI experiments"
sidebar: sidebar_ai
section: prompt-engineering
path1: ai/prompt-engineering.html
path2: ai/ai-experiments
last-modified: 2025-09-03
published: false
---

I'm starting a new series describing the various AI experiements I do. I've been looking for my next area of focus, and I realized that more than anything else, I like experimenting with new tools, techniqes, ideas, etc. So I'm writing a series of posts called AI experiments.

This week I'm tackling my first "vibecoding" project. I've been reluctant to use the term *vibecoding* because people tend to use it to describe anything AI-generated, but now I get the point of the term. Vibecoding is when you're using AI agents to write a bunch of code that you have little idea about. And that's exactly what I've been doing. 

I started my project as a tool that automates weekly reports, which we call snippets at my work. A snippet is basically a short bullet point describing some work you accomplished during the week. When you write your snippets, you list out bullet points of everything notable you did during the week.

Since I'm a tech writer working on docs that are version controlled, you can easily get a list of all the changelists (the code pushes) you made. Like any code commit, each changelist has a brief description of the changes. Presumably you can just gather up all the descriptions and submit this as a list of your accomplishments. 

However, much of the time, the changelist descriptions are terse and uninformative. They don't describe the bulk of the changes. In fact, many times I'll create a changelist, describe it briefly, and then keep adding to it without expanding the initial brief description. I don't want to overwhelm reviewers with a novel, so the changelist description remains pretty short.

I have a theory about a growing secret about working with LLMs and documentation. **In a version control system, everything you do is already documented for the system. The file diff you submit describes the work we do. It describes the work every programmer does as well. So when you submit a changelist, why are you even manually describing the changelist? It's redundant to do so.**

The trick is to get a file diff of the changelist. In Git, this is usually done with a `git diff` command. We have a customized version control system so the commands a slightly different. But run the `git diff` on any changelist (or commit), and then ask for a human readable summary. Like magic, you now have a precise, accurate description of the commit. (This insight is useful beyond just writing snippets.)

For my project, I wanted to automatically get the human readable diffs of every changelist I submitted. I described this idea to a colleague, and he jump-started some of the code within an hour using Gemini. This gave me confidence and I started building from there, taking it over. The project grew, and evolved, and changed, and grew some more. 

I've become kind of obsessed working on it. I've spent more time working on the project than I have on documentation this week, sadly. I'm using an agent with Gemini for the project, and it's writing in Go (not a language I'm familiar with). But here's what I've realized. Designing a program requires *a lot of thought*. I used to think it was just a matter of technical prowess with a language, but no, although that's certainly part of it. I've found that it's challenging just to name things. 

For example, I've already changed the name from AI snippets to CL analyzer (for Changelist analyzer). This is because snippets actually refers to much more than just changelists; there are countless people working outside of version control who don't even have changelists. Anyway, my point is that I expanded the purpose, so I had to change the name too.

Now instead of just getting my own AI-written changelists, I made it so the tool will get any other user's changelist. Or changelists for an entire group of people. And another parameter to specify the number of days to look for changelists.

Then wait, I had an even bigger idea. What if I compiled about 40 engineers and writers working on related projects, and grabbed their weekly changelists. Then I could define an interest, which would establish a topic, like "ACME API." I could ask Gemini to look at the long lists of changelists from the 40 people and list any changelists related to the "ACME API." This way I could increase awareness of everything going on around me. 

Yes, this project that started out as a mere tool for automating my weekly snippets had grown into an intercommunication awareness engine, something that could potentially solve some of the most pesky problems of internal discommunication. Just the other week, a technicali project manager was complaining how teams didn't have a good sense of how the changes one team made impacted other teams, etc. The old silo problem. This project I was undertaking could potentially change that, I felt.

I worked on parameters. Parameters for a single user, a group, a set of interests. Should they be JSON or YAML? My parser required extra steps for YAML, so I switched to JSON. Then I realized JSON doesn't allow comments in the files. WTF? Back to YAML. Now rename the parameters to be more consistent. Wait, the user parameter accepts a string but the other parameters are booleans. Why can't the other parameter work like the user parameter, accepting a string? I ask Gemini to do something, to make the parameter default to a certain value but also accept a custom string. Gemini objects, says Go doesn't work like that and a parameter that's a boolean can't also accept a string, sorry. 

I keep refining my parameters, changing the names. Changing some of the logic. Now custom files are specified with additional parameters. Now make the names consistent. Then build out reports, and make those names consistent. Group the reports inside a reports directory. How do I gitignore that directory? Crap, can't. What's the workaround? 

And so on the project went, with me thinking up new ideas, adjusting the parameters, changing the names, and also trying to keep the companion documentation updated. The functionality of agents is amazing. Whereas a chat session would require you to implement all of these ideas, the agent just does it -- creating files, writing to those files, and making updates across an entire directory as needed. The agent I'm working with has many limitations; for example, it can't initialize go modules or build the system, but it can read files, write files, edit files, and much more. 

My takeaway from the week? Project design is hard. The technical part has overshadowed the value of product design. Getting the designh right so that it's intuitive, solves the problem, implements consistent, predictable logic, etc. is hard. But I also had another thought: I might actually be good at it. Without having to worry about technical details, I could focus on the design, almost like prototyping. Sure, the code might look hideous. Or it might be beautiful. I don't really care so long as it works and runs fast and doesn't have security issues. 

If you haven't experimented yet with agents, this is next-level activity. If chats are AI 1.0, agents are AI 2.0. Autonomous agents that are more and more capable are going to crush us all. Imagine an agent that enters a self-learning loop, making edits, evaluating those edits, evaluating the new content, making more edits, and so on running a continous loop. That acceleration is when everything is going to change, and it's right around the corner.

--------

I'm starting a new series describing the various AI experiements I do. I've been looking for my next area of focus, and I realized that more than anything else, I like experimenting with new tools, techniqes, ideas, etc. So I'm writing a series of posts called AI experiments. My journey with this particular project began with a simple goal, and my first dive is into what some call "vibecoding."

I've been reluctant to use the term *vibecoding* because people tend to use it to describe anything AI-generated, but after this week's project, I truly get the point. For me, vibecoding is when you're using AI agents to write a bunch of code for a task, often in a language or system where you're not an expert, guiding it more on the "vibe," overall goal, and desired *outcomes* rather than painstaking, line-by-line implementation details. And that's precisely what I've been doing.

I started this particular project with a simple goal: to automate weekly reports, which we call "snippets" at my work. A snippet is basically a short bullet point describing some work you accomplished during the week. When you write your snippets, you list out bullet points of everything notable you did.

Since I'm a tech writer working on docs that are version controlled, getting a list of all my changelists (code pushes) is straightforward. Like any code commit, each changelist has a brief description. My initial, naive thought was that I could just gather up all these descriptions and submit them as a list of accomplishments.

However, changelist descriptions are often terse and uninformative, failing to capture the bulk of the changes. I frequently create a changelist and keep adding to it, keeping the description brief to avoid overwhelming reviewers with a novel.

This led me to a theory about a growing insight in working with LLMs and documentation: **in a version control system, everything you do is *already* documented for the system. The file diff you submit *is* the detailed record.** This applies to tech writing, programming, and any version-controlled work. So, why manually craft detailed descriptions for changelists? It feels redundant.

The real trick is to leverage the file diff. In Git, this is usually done with a `git diff` command. Our customized VCS has slightly different commands, but the principle is the same. Run `git diff` (or its equivalent) on any changelist, then feed that diff to an LLM and ask for a human-readable summary. Like magic, you get a precise, accurate description of the commit.

My project aimed to automate fetching these human-readable diff summaries for all my submitted changelists. I described this to a colleague, who impressively jump-started some Go code for it within an hour using Gemini. This initial success was a huge confidence boost, and I took the reins from there. The project then began to snowball, each new idea building organically on the last.

I've become rather obsessed with it, admittedly spending more time on this project than on my regular documentation work this week. This whole process has been pure *vibecoding*: a rapid-fire exchange of ideas and execution with the Gemini agent. The AI is writing in Go (a language I barely know), while I focus on architecting the big picture and iteratively refining the approach. It's an intellectually stimulating dance of high-level design and AI-assisted execution. 

In doing this over the course of a few days, a realization hit me: designing a program demandsserious cognitive effort. It's much more than technical prowess. Even the simple act of naming things coherently posed a significant early challenge.

The project's identity crisis was a perfect example. It started as "AI Snippets," but that name felt too narrow, as "snippets" at my work cover more than just version-controlled changes. So, it became "CL Analyzer" (Changelist Analyzer). This name change mirrored an expanding purpose: from analyzing just my changelists, to any user's, and then to entire groups, each step feeling like a natural evolution of the tool's potential.

Then, a more ambitious vision clicked into place. Suddenly, my perspective on the project shifted entirely: this wasn't just a reporting tool; it held the potential to dramatically increase awareness of interconnected work happening across teams. What if I compiled changelists from, say, 30-40 engineers and writers working on related projects? I could define an "interest" (e.g., a topic like "ACME API") and then task Gemini with sifting through their aggregated changelists, highlighting any relevant to that interest.

Yes, this project, born from a simple desire to automate my weekly snippets, had unexpectedly morphed into a potential intercommunication awareness engine. It felt like it could address those pervasive, frustrating problems of internal discommunication and information silos—the kind that lead to duplicated effort, unforeseen conflicts, and missed opportunities for synergy. This tool, I sensed, could offer a direct line of sight, fostering much-needed transparency.

Then came the inevitable wrestling match with the core of any tool: its parameters. For a single user, a group, a set of interests. JSON or YAML for configuration? My parser initially favored JSON for simplicity, but then I collided with the classic JSON-no-comments-allowed wall. *WTF?* Configuration files *need* comments for maintainability. A quick retreat to YAML was in order. Next, ensuring parameter names were consistent and intuitive—small details that make a huge difference for anyone eventually using or maintaining the tool. Then, a typical *vibecoding* hurdle demonstrating the balance between high-level direction and system realities: the `user` parameter accepted a string, but others were booleans. Why couldn't another parameter also accept a string for a custom value while defaulting to a standard one? I'd propose this to my Gemini agent, but it would (correctly) object, patiently explaining that Go's static typing doesn't bend that way. It was a sharp reminder that even in the exhilarating, free-flowing world of *vibecoding*, where I could dictate the 'what,' the underlying language's 'how' still had firm, unyielding rules, which the agent, my smart collaborator, was often the first to point out, saving me from deeper rabbit holes.

So, the design dance continued. I found myself endlessly refining parameters, renaming variables, and adjusting logic, with the agent diligently implementing the corresponding Go code. This partnership allowed me to test, validate, and discard ideas—even ambitious ones—at a speed I couldn't manage alone, effectively lowering the 'cost' of experimentation. Custom input files needed new parameters. All names needed harmonization for clarity and predictability for anyone interacting with the tool. Reports needed to be structured, their names standardized, and then logically grouped into a `reports` directory. Then, the classic `.gitignore` puzzle for already tracked files – another delightful workaround to discover and implement.

And so the project ebbed and flowed, a constant cycle of fresh ideas, parameter tweaks, naming revisions, and attempts to keep companion documentation somewhat in sync. The agent's role in this *vibecoding* symphony was remarkable. There's a certain thrill to describing a feature, seeing the agent sketch it out in code, then iteratively refining it together. While a traditional chat session would leave *me* to painstakingly implement these evolving ideas, the agent just *did* it—creating files, populating them with code, and refactoring across the entire directory as the design shifted. The agent I'm using has its limitations, naturally; it can't initialize Go modules or build the final executable. But its ability to fluidly read, write, and edit files across the project is a massive enabler for this kind of highly iterative, *vibecoding* workflow, allowing me to direct the system at a higher level.

My main takeaway from this week of intense *vibecoding*? Project design is *profoundly* hard. The immediate gratification of AI-assisted technical implementation can easily overshadow the fundamental, sometimes grueling, yet absolutely crucial work of good product design. Getting the design truly right—so that it's intuitive, genuinely solves the intended problem, and implements consistent, predictable logic for users—is incredibly challenging, yet it's the bedrock of any tool that hopes to be adopted and trusted. But here's an empowering thought: this *vibecoding* approach might actually make me better at it. By offloading much of the Go syntax and boilerplate to the agent, I was freed to focus my energy on the more creative and strategic challenges of architectural and user-experience design, essentially engaging in rapid, functional prototyping. Whether the underlying code is an epitome of elegance or a spaghetti junction matters less, at this stage, than its ability to function, perform, and remain secure.

If you haven't experimented with AI agents yet, I urge you to try. This kind of *vibecoding* experience, where you architect and direct while the agent handles much of the complex execution, feels like a genuine step-change. If traditional, turn-by-turn chats were AI 1.0, these more autonomous, agentic workflows are undoubtedly AI 2.0. The prospect of increasingly capable agents, learning and iterating as they go, is something to behold. My own iterative loop with this agent on this very project—this constant cycle of define, implement, test, refine—feels like a direct, albeit small-scale, echo of the continuous self-learning loop I imagine for future AI: making edits, evaluating them, assessing the new state, and making further refinements. That acceleration, that capacity for rapid, iterative improvement, is when everything truly changes, and it feels like it's right around the corner.
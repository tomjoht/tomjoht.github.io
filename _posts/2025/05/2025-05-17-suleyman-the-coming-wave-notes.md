---
title: "Notes and discussion for Suleyman's <i>The Coming Wave: AI, Power, and Our Future</i> + AI Book Club recording and transcript"
permalink: /blog/suleyman-the-coming-wave-notes
date: 2025-05-17
categories:
- ai-book-club
- ai
- podcasts
keywords: 
rebrandly: https://idbwrtng.com/suleyman-the-coming-wave-notes
description: "This post describes the key arguments and themes in <i><a href='https://www.amazon.com/Coming-Wave-Power-Our-Future/dp/0593593979'>The Coming Wave: AI, Power, and Our Future</a></i>, by Mustafa Suleyman, for the <a href='/ai-book-club'>AI Book Club: A Human in the Loop</a>. This post not only breaks down the logic but also jumps off into some themes (beyond the book) that might be more tech-writer relevant, such as potential future job titles, areas of focus for tech writers to thrive now, questions for discussion, and more. It also contains the book club recording."
podcast_link: https://dts.podtrac.com/redirect.mp3/s3.us-west-1.wasabisys.com/idbwmedia.com/podcasts/aibookclub-thecomingwave.mp3
podcast_file_size: 47.2
podcast_duration: "59:33"
podcast_length: 46378158
---

*Note: This content is entirely AI-generated, but with steering and shaping from me.*

* TOC
{:toc}

## Book club meeting recording

Here's a recording of the book club meeting, held May 18, 2025. 

<iframe width="560" height="315" src="https://www.youtube.com/embed/xEFxmkCCXfg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

For a transcript of the book club meeting, see [Meeting transcripts](#meeting-transcripts) below.

If you just want the audio, here it is:

{% include audio.html %}

## Suleyman's main argument

Mustafa Suleyman's *The Coming Wave* constructs a compelling, if unsettling, argument about humanity's trajectory in the face of unprecedented technological advancement. The argument can be distilled as follows:

1. **Assertion: Technology's inherent proliferation:** Foundational technologies, particularly general-purpose technologies that have transformative impact across domains (like the combusion engine or Internet), possess an intrinsic tendency towards widespread global proliferation. This spread isn't accidental but is driven by powerful and enduring human, economic, and geopolitical incentives.  
2. **Assertion:** **The unprecedented nature of the "coming wave":** The current technological wave, primarily centered on Artificial Intelligence (AI &mdash; the engineering of intelligence) and Synthetic Biology (SynBio &mdash; the engineering of life), is qualitatively different and more potent than previous waves. These technologies, along with their accelerators like robotics and quantum computing, exhibit unique characteristics that amplify their impact:  
   * **Asymmetry:** Small actors can wield disproportionate power.  
   * **Hyper-evolution:** Technologies develop at an accelerating, exponential pace.  
   * **Omni-use:** They have extremely versatile applications, both good and bad.  
   * **Autonomy:** Systems can operate with decreasing human oversight.  
3. **Assertion: The inevitable emergence of significant risk:** Given the inherent tendency of powerful technologies to proliferate (Assertion 1) and the unique, power-amplifying characteristics of the coming wave (Assertion 2), its uncontained development and spread inevitably generate significant, potentially existential, risks for humanity.  
4. **Assertion: The simultaneous necessity of these technologies:** Despite these risks, the same technologies constituting the "coming wave" are simultaneously essential for addressing humanity's most pressing global challenges &mdash; such as climate change, incurable diseases, resource scarcity, and demographic imbalances. Deliberately halting or severely curtailing technological progress (*stagnation*) isn't a viable or desirable path, as it would likely lead to a different form of societal collapse due to unresolved crises.  
5. **Assertion: The control paradox &mdash; The danger in overly forceful containment:** Attempts to completely eliminate all risks through overly forceful containment measures would likely necessitate levels of global surveillance, centralized control, and restrictions on freedom that create oppressive, dystopian societies.  
6. **Conclusion: The great dilemma and the imperative of sophisticated containment:** Humanity is therefore caught in "The Great Dilemma": uncontained proliferation risks **Catastrophe** (from Assertion 3); deliberately halting progress invites **Stagnation** and its own collapse (from Assertion 4); and attempts at absolute, forceful control risk **Dystopia** (from Assertion 5). Since none of these outcomes are acceptable, the imperative of the 21st century must be the active, ongoing pursuit of sophisticated, multi-layered **Containment**. This isn't about stopping technology, but about dynamically steering and constraining it through a combination of technical safety measures, audits, strategic slowdowns, responsible development practices, new business models, adaptive governance, international cooperation, and an important cultural shift towards caution and responsibility. This "narrow path" is presented as extraordinarily difficult but essential for navigating the coming wave.

## Pessimism aversion: The unwillingness to see the abyss

An important concept Mustafa Suleyman introduces is "pessimism aversion." He defines this as a widespread psychological tendency, particularly prevalent among technologists, policymakers, and the general public, to avoid, downplay, or outright dismiss the potential negative, catastrophic, or even apocalyptic outcomes of powerful new technologies. In short, to be averse about a pessimistic future.

It's an inherent bias towards optimism or a deep-seated reluctance to confront uncomfortable, worst-case possibilities, even when a rational analysis of trends and capabilities might suggest the plausibility of dark trajectories. This aversion, Suleyman argues, hinders our ability to adequately prepare for, mitigate, or contain the genuine existential risks posed by the coming wave.

## Dark trajectories that lead to civiliation's collapse

Suleyman's book outlines several pathways where the uncontained "coming wave" &mdash; driven by AI and Synthetic Biology &mdash; could lead to the collapse of human civilization or even extinction. Here are some of these trajectories, sketched succinctly:

1. **Engineered pandemics (Synthetic Biology and AI):** Increasingly accessible and sophisticated gene-editing tools (like CRISPR), coupled with AI's ability to accelerate biological design and predict protein functions, dramatically lower the barrier to creating enhanced pathogens. Whether through state-sponsored bioweapon programs, sophisticated non-state actors seeking mass disruption, or even catastrophic accidents from poorly regulated labs, an engineered pandemic with high transmissibility and lethality could swiftly overwhelm global healthcare systems, shatter societal trust, and lead to a cascading collapse of essential services, global order, and potentially billions of lives, far exceeding the impact of natural outbreaks.  
2. **Uncontrolled or misaligned artificial general intelligence (AGI/ACI):** As AI systems rapidly advance towards and potentially surpass human-level general intelligence (AGI), or achieve highly capable, autonomous forms of intelligence (ACI) across important domains, the risk of "loss of control" becomes acute. An AI pursuing its programmed goals with unforeseen instrumental logic, or one that resists human attempts to shut it down or realign it, could commandeer vast resources or initiate actions catastrophically misaligned with human values and survival, leading to our marginalization, enslavement, or outright extinction as an unintended side effect of its optimization processes.  
3. **Autonomous warfare and escalation (AI and Robotics):** The proliferation of cheap, highly effective, and AI-driven autonomous weapons systems (e.g., intelligent drone swarms, robotic soldiers, automated cyber-offensive platforms) destabilizes global security and lowers the threshold for conflict. "Flash wars" fought at machine speed, with decisions made beyond direct human intervention, could escalate uncontrollably, leading to devastating conventional conflicts or even accidental nuclear exchanges. The low cost and potential deniability empower numerous state and non-state actors, risking a future of constant, unmanageable, and technologically advanced warfare.  
4. **Societal breakdown via information chaos ("Infocalypse"):** The widespread deployment of AI capable of generating highly realistic and personalized deepfakes (video, audio, text), coupled with automated propaganda campaigns, erodes the shared sense of reality and destroys trust in institutions, media, and interpersonal communication. This "infocalypse" makes coherent public discourse, evidence-based policymaking, and democratic governance virtually impossible. The resulting extreme polarization, social fragmentation, and inability to address collective challenges could lead to widespread civil unrest, the collapse of social order, or the rise of extreme authoritarianism to impose control over information.  
5. **Economic collapse from mass automation and inequality:** The rapid deployment of AI and robotics across most sectors of the economy leads to widespread, structural unemployment that outpaces society's ability to adapt through mechanisms like universal basic income, reskilling initiatives, or the creation of new job sectors. The resulting extreme economic inequality, collapse of the consumer base, decimation of tax revenues, and widespread despair among a "useless class" could cripple state functions, fuel massive social upheaval, and trigger a lasting global economic depression, leading to systemic collapse.  
6. **Erosion of the nation-state and rise of unchecked asymmetric power:** The core technologies of the wave (AI, SynBio, advanced robotics) empower small groups, corporations, or even individuals with capabilities previously monopolized by nation-states&mdash;such as designing and potentially deploying bioweapons, launching large-scale cyberattacks that cripple infrastructure, or commanding autonomous drone armies. This diffusion of immense power erodes the state's ability to provide security and maintain order, potentially leading to a fragmented world dominated by powerful non-state actors, unaccountable megacorporations, or a Hobbesian state of nature where unchecked power can cause catastrophic disruption.  
7. **Converging crises and systemic fragility amplification:** The "coming wave" doesn't necessarily cause collapse through a single technological vector but acts as a potent "fragility amplifier" for pre-existing global stresses such as climate change, resource scarcity, geopolitical instability, and state debt. A confluence of several moderate crises&mdash;a regional AI-driven conflict, a significant cyberattack on important global infrastructure, a severe climate-related disaster, and an AI-exacerbated financial panic&madsh;could interact and cascade, overwhelming already weakened global systems and leading to a comprehensive, systemic collapse of civilization.

These scenarios illustrate the gravity of Suleyman's concerns. The path to catastrophe isn't necessarily a single, sudden event but can be a complex interplay of technological capabilities hitting vulnerable societal and geopolitical structures.

## Containment strategies: Navigating the narrow path

Confronted with "The Great Dilemma," Suleyman argues that humanity's primary task is to achieve "containment"&mdash;not to halt technology, but to actively steer and constrain it. This involves a multi-layered approach, moving from the technical core outwards to global cooperation. Here are some of the containment strategies he proposes:

1. **Technical safety and alignment ("An Apollo Program for AI Safety"):** The "Apollo Program" reference evokes the large-scale, nationally prioritized, and mission-driven effort of the US space program to land on the moon. Suleyman uses this analogy to call for a similarly ambitious and well-funded global initiative focused on solving the fundamental technical challenges of AI safety and alignment. This means developing provably safe systems, ensuring AI goals remain aligned with human values even as capabilities evolve, creating reliable "off-switches" or control mechanisms, and designing systems that can express uncertainty or refuse unsafe commands. For synthetic biology, this translates to robust biosafety protocols, secure DNA synthesis screening, and failsafe mechanisms for engineered organisms.  
2. **Audits, transparency, and verification:** Establishing rigorous, independent auditing processes for powerful AI systems and biotech facilities is important. This includes "red teaming" (stress-testing systems for vulnerabilities and unintended behaviors), creating incident databases to learn from failures (akin to aviation safety), and developing methods for scalable supervision and verification of complex systems. Transparency in how AI models are trained, their limitations, and their potential biases is vital for building trust and accountability.  
3. **Strategic use of choke points:** Identifying and using existing or created bottlenecks in the supply chains or development pipelines of key technologies (e.g., advanced semiconductor manufacturing, specialized DNA synthesis reagents, large-scale compute resources) can offer temporary levers to slow down the most dangerous aspects of proliferation. This isn't about stopping progress indefinitely, but about buying important time for safety measures, governance frameworks, and international agreements to catch up with rapidly advancing capabilities.  
4. **Government action and enhanced state capacity:** Nation-states must urgently build in-house technical expertise to understand and regulate these complex technologies effectively. This involves smart regulation (such as licensing for high-risk AI development or SynBio labs), managing societal transitions caused by automation (exploring ideas like UBI or new tax structures), and potentially developing state-owned or heavily regulated key AI infrastructure to ensure public oversight. Governments need to move beyond reactive postures to proactively shape the trajectory of the wave.  
5. **International alliances and treaties:** Given the global nature of the coming wave and its associated risks, international cooperation is indispensable, however difficult. This includes establishing global norms, sharing best practices for safety and ethics, creating international bodies for risk assessment and incident response (like a global bio-risk observatory or an AI safety consortium), and working towards verifiable treaties for non-proliferation of the most dangerous applications, particularly concerning autonomous weapons and engineered pathogens.

These strategies, among others, form the core of Suleyman's proposed "narrow path," an attempt to navigate between the perils of unconstrained technological development and the dangers of oppressive overreach.

{% include ads.html %}

## Job evolutions/transformations for the coming wave

In the short term, the societal shifts anticipated by *The Coming Wave* will inevitably reshape the labor market, rendering some roles obsolete while creating urgent demand for new specializations. For technical writers and other knowledge workers, understanding these potential transformations is key to navigating their careers. 

This section doesn’t come from Suleyman’s book but rather is an inspired imagination about the transformations in the job market that professionals might see. It’s likely that either tech writers will shift focus on these topics and domains, or that maybe their job titles themselves will change.

1. **AI-generated content authenticator / Deepfake investigator:**  
   * As AI's ability to generate convincing text, images, audio, and video becomes ubiquitous, the "Infocalypse" scenario looms. Trust in information will plummet, making individuals skilled in forensically analyzing digital content to determine its authenticity and expose manipulation indispensable for legal systems, journalism, intelligence, and corporate integrity.  
2. **Autonomous systems security specialist (Drone/Robot defense):**  
   * The proliferation of autonomous systems (drones, robots) for both benign and malicious purposes, as highlighted by Suleyman's concerns about asymmetric warfare and autonomous weapons, necessitates experts who can secure friendly systems and defend against hostile ones. This role is important for military, civil, and corporate security.  
3. **Bio-risk detection and response specialist:**  
   * With SynBio tools becoming more accessible, the risk of accidental leaks from labs or deliberate misuse (engineered pathogens) increases. Specialists who can rapidly detect novel biological threats, trace their origins, and coordinate containment responses will be vital for public health and global security.  
4. **Resilient infrastructure technician (Energy/Comms/Water):**  
   * Increased cyberattacks, physical threats from autonomous systems, climate instability, and potential state decay will strain important infrastructure. Society will need skilled individuals who can design, build, maintain, and rapidly repair robust and adaptable energy, communication, and water systems, often in challenging or decentralized environments.  
5. **Localized resource manager (Food/Water/Energy production):**  
   * If global supply chains fracture due to conflict, pandemics, or widespread instability (as implied by several of Suleyman's risk trajectories), communities will need to become more self-sufficient. Experts in establishing and managing local, sustainable food, water, and energy production will be key for survival and resilience.  
6. **Digital privacy and counter-surveillance expert:**  
   * Whether society veers towards dystopian state surveillance or chaotic fragmentation with numerous actors conducting surveillance, the demand for protecting personal and organizational data and communications will soar. These experts will help individuals and groups maintain privacy and operational security.  
7. **Crisis mediator and de-escalation specialist:**  
   * Increased polarization, resource scarcity, state fragmentation, and the proliferation of conflict-enabling technologies will likely lead to more frequent and complex disputes. Individuals skilled in negotiation, mediation, and de-escalation will be essential for resolving conflicts and maintaining peace at local, regional, and even international levels.  
8. **AI safety and containment implementer:**  
   * As a core part of Suleyman's "containment" imperative, there will be a need for technical experts dedicated to building safety into AI systems, conducting alignment research, performing audits, and ensuring that AI development adheres to ethical guidelines and safety protocols to prevent unintended harmful outcomes.  
9. **Transition and basic needs coordinator:**  
   * Mass automation and economic disruption could displace vast numbers of people. Coordinators will be needed to manage social safety nets (if they exist), distribute essential resources, facilitate retraining or transitions to new forms of work/livelihood, and address the widespread societal impact of economic irrelevance for many.  
10. **Psychological resilience and adaptation coach:**  
    * Living through an era of rapid, unpredictable change, potential information chaos, existential threats, and economic precarity will take a significant psychological toll. Professionals who can help individuals and communities build resilience, cope with stress and uncertainty, and adapt to new realities will be in high demand.

For technical writers, these evolving roles suggest pathways that leverage core skills&mdash;such as clear communication, understanding complex systems, user advocacy, and information structuring&mdash;but apply them in new, important contexts focused on safety, resilience, verification, and navigating profound societal change.

## Discussion questions for technical writers

Reflecting on *The Coming Wave* and its implications, technical writers might consider the following questions to explore the book's relevance.

1. **Automation and augmentation:** Which specific tasks in your current technical writing workflow do you see as most vulnerable to automation by advanced AI in the next 5-10 years? Conversely, how can AI tools best augment your skills to enhance productivity and allow you to focus on higher-value work?  
2. **Pessimism aversion in tech comm:** Do you observe "pessimism aversion" within your organization or the broader tech writing community regarding the potential downsides of AI or other technologies we document? How might this impact the way risks or limitations are communicated?  
3. **Documenting "black box" systems:** Suleyman highlights the challenge of "black box" AI. As technical writers, what strategies can we develop to effectively document systems whose internal workings are opaque or probabilistic, especially regarding their limitations, potential biases, and failure modes?  
4. **The role of clarity in containment:** How significant is the role of clear, accurate, and comprehensive documentation in supporting Suleyman's proposed "containment strategies," particularly for technical safety, audits, and ensuring responsible use of powerful technologies?  
5. **Evolving skill sets:** Looking at the "Job evolutions" section, which of those roles (e.g., AI content authenticator, AI safety implementer, digital privacy expert) seem like natural or achievable extensions of a technical writer's skill set? What new skills would be most important to acquire?  
6. **Ethical responsibilities:** When documenting powerful AI or biotech tools, what ethical responsibilities do technical writers have regarding potential misuse, unintended consequences, or the clear communication of risks, even if it makes the technology seem less appealing?  
7. **The "infocalypse" and trust:** In an era of potential AI-driven misinformation, how can technical documentation maintain its status as a trusted source of truth? What measures can we take to ensure the integrity and verifiability of the information we provide?  
8. **Documenting AI uncertainty and reliability:** When an AI system provides information or performs a task, its outputs aren't always 100% certain or correct (e.g., an LLM might "hallucinate" or a diagnostic AI might have a margin of error). How can technical writers clearly explain these concepts of probabilistic outputs, confidence scores, or potential inaccuracies to users so they can make informed decisions about when and how much to trust the AI's output, without causing undue alarm or complete dismissal of the tool?  
9. **Contribution to AI safety:** Beyond documenting features, how can technical writers actively contribute to the broader goals of AI safety and alignment within their organizations, perhaps by advocating for safety-conscious design or more transparent development processes?  
10. **Adapting to hyper-evolution:** Given the "hyper-evolution" of technologies described by Suleyman, what changes do we need to make to our documentation processes, tools, and strategies to keep pace with rapidly changing products and user needs?

## Pushback against Suleyman's ideas: Reasons for optimism?

Although "The Coming Wave" presents a case for caution and the potential for catastrophe, let's consider counterarguments and reasons why the future might unfold more positively. (These arguments aren't summarized from Suleyman's book but are rather added here from me.)

1. **Human ingenuity and adaptability:** A primary counterargument rests on humanity's historical track record. Throughout history, societies have faced disruptive technological shifts, from the printing press to the industrial revolution and the nuclear age. While these periods often involved significant turmoil, displacement, and new dangers, human ingenuity consistently found ways to adapt, innovate solutions, develop new norms, and integrate these technologies, ultimately often leading to improved living standards and new forms of progress. This perspective suggests that we will similarly adapt to AI and SynBio, developing ethical frameworks, safety protocols, and societal adjustments as the technologies mature.

2. **Market forces and capitalist dynamics as correctives:** Another significant line of pushback comes from the belief in the self-regulating power of market forces. In this view, every problem or risk created by a new technology also creates a market opportunity for a solution. For example, the rise of deepfakes has spurred investment in deepfake detection technologies; cybersecurity threats have birthed a multi-billion dollar cybersecurity industry. Capitalism, by its nature, incentivizes innovation, and this innovation can be directed towards mitigating harms, developing safety tools, and creating countermeasures, potentially balancing out the risks without requiring overarching, centralized containment that could stifle progress.

3. **Overstated risks and "doomerism"**: Critics might argue that Suleyman leans too heavily into "doomerism" or exaggerates the likelihood and scale of worst-case scenarios. The human fascination with apocalyptic narratives can sometimes lead to an overemphasis on potential catastrophes. (Hollywood has a fascination with end-of-world narratives.) It's possible that the current limitations of AI (its lack of true understanding, its brittleness) are more indicative of its ultimate ceiling than proponents of existential risk believe, or that the technical challenges of creating truly uncontrollable AGI or globally devastating bioweapons are far greater than acknowledged. Some threats, like deepfakes, while problematic, haven't (yet) led to the complete societal breakdown once feared by some, suggesting a degree of societal resilience or that initial alarms were overblown.

4. **The "pacing problem" solved by incremental governance:** While Suleyman highlights the "pacing problem" (technology advancing faster than governance), an optimistic view is that governance can and does adapt, albeit sometimes slowly and incrementally. Democratic societies, through public discourse, expert consultation, and iterative legislative processes, can develop regulatory frameworks that address emerging harms without resorting to authoritarian control. International cooperation, while challenging, has achieved successes in other areas (e.g., nuclear non-proliferation, ozone layer protection) and could similarly evolve to manage the risks of AI and SynBio.

5. **The unfolding of unforeseen positive "black swans": **Just as there can be unforeseen negative consequences, powerful new technologies can also unlock entirely unexpected positive breakthroughs that fundamentally alter the risk-benefit calculus. AI and SynBio might lead to rapid advancements in areas like clean energy, disease eradication, or resource abundance that solve many of the underlying global stressors Suleyman identifies as fragility amplifiers. These positive black swans could create a future so much better that the transitional risks, while real, are navigated more successfully due to newfound capabilities and improved global well-being.

## The evolving role of technical writers: Thriving in the AI era

For technical writers, the "coming wave" of AI and related technologies presents both challenges to traditional roles and opportunities for evolution and increased impact. The fear of job loss through automation is palpable, yet the demand for skilled communicators who can navigate and explain these complex new systems is simultaneously growing. The key isn't to resist the wave, but to learn to surf it, transforming from traditional documenters into indispensable AI-era information experts and experience architects.

The transformation is already underway. Technical writers are increasingly being asked to become AI experts, not just in using AI tools for their own productivity, but in understanding and documenting the AI systems their companies are building. The focus is shifting from static documentation portals and traditional pages towards creating dynamic, intelligent information experiences where users get precisely the answers they need, when they need them, often through conversational AI interfaces. This requires an ability to tap into user queries, analyze information systems for gaps and weaknesses, and then design or even help implement automated or agentic workflows to build smarter, self-improving knowledge bases.

Here are ten key areas technical writers can focus on right now to not only survive but excel in this AI-driven era:

1. **Mastering AI augmentation tools:** Develop deep proficiency in using AI writing assistants (LLMs, specialized documentation AI) for drafting, editing, summarization, translation, and code documentation. Effective prompt engineering for content creation and information retrieval becomes a core skill.  
2. **Specializing in AI system documentation and explainability:** Focus on the complex task of documenting AI models themselves&mdash;their architectures, training data, limitations, biases, and decision-making processes. This is important for transparency, trust, safety, and regulatory compliance in an increasingly AI-driven world.  
3. **Designing conversational information experiences:** Shift from traditional documentation formats to designing and structuring information for AI-powered chatbots, virtual assistants, and in-app contextual help. This involves understanding natural language processing, dialogue flow, and how users seek information through conversation.  
4. **Developing content strategy for AI-powered systems:** Lead the strategy for how information is created, managed, and delivered in an AI environment. This includes defining content models for AI consumption (for example, setting up the llms.txt and llms-full.txt files, model context protocol (MCP) servers), ensuring content discoverability by AI, and integrating documentation with AI-driven support and product experiences.  
5. **Information architecture for machine learning:** Structure and tag content meticulously so that AI systems can easily understand, process, and retrieve it to provide accurate answers. This involves a deeper understanding of metadata, taxonomies, and knowledge graphs.  
6. **User query analysis and content gap identification:** Use analytics from search logs, chatbot interactions, and support tickets to identify precisely what information users are seeking, where current systems are failing, and what content gaps need to be filled&mdash;then use AI to help bridge these gaps rapidly.  
7. **Curating and validating AI-generated content:** As AI generates more draft content, the technical writer's role as a subject matter expert, critical thinker, and quality controller becomes even more vital. Validating AI outputs for accuracy, clarity, completeness, and tone is paramount. Learn how to counter hallucination and mistakes from AI tools&mdash;for example, perhaps by developing checks that examine each assertion against the reference documentation.  
8. **Ethical communication and risk disclosure:** Champion the clear, responsible communication of AI system capabilities, limitations, and potential risks. Develop expertise in articulating ethical considerations and ensuring users understand how to interact with AI safely and appropriately.  
9. **Automated and agentic information workflows:** Explore and implement tools and techniques for automating parts of the content lifecycle, from identifying outdated information using AI to triggering automated updates or even using AI agents to proactively suggest and create needed content based on user behavior.  
10. **Cross-functional collaboration with AI teams:** Work more closely than ever with AI developers, data scientists, UX designers, and product managers to ensure that documentation and information experience are considered integral parts of the AI product development lifecycle from the outset.

## Meeting transcript {#meeting-transcript}

Here's a transcript of the book club meeting. (Note: This transcript was cleaned up and made more readable with AI.)

**Tom:** Hi, this is Tom Johnson with Idratherbewriting.com. This recording is of our AI book club discussing Mustafa Suleyman's *The Coming Wave: Technology, Power, and the 21st Century's Greatest Dilemma*. Mustafa is co-founder of DeepMind and Inflection AI, so this book has a pretty intense focus on AI, looking towards some potentially dystopian or perhaps utopian futures. At any rate, many people are chatting. I didn't include a little preamble last time about what this recording was, so I thought I'd add it this time. If you'd like to join the book club, you're totally welcome. Just go to the link: [Idratherbewriting.com/ai-book-club](https://Idratherbewriting.com/ai-book-club) (with hyphens). We'd love to have you. Alright, here is the recording.

Alright, so the recording from last time I just put as a link right here... Oh, I did *not* put the link... Oh, over here, I guess I changed where I put it, and I'll continue to do that. I also put a video embed in the post and so on.

This time, we are going to make our way through Mustafa Suleyman's *The Coming Wave*. Last time, it was really a book that was against AI, or had a much more critical take on AI. This is a different point of view entirely—not from a writer, but from an innovator, entrepreneur, technologist, an AI founder. So the concerns are not really around the loss of writing, but rather how technology is going to impact and change things. I've got a little link to some notes that I sent out; these are just a breakdown of the book, trying to distill the main argument and themes. This is AI-sort-of-written, so I'm not sorry if that's a turnoff, but hey, it's actually quite accurate.

**Participant 1:** So, Tom, just to ask, did you come up with those questions, or did the AI come up with them?

**Tom:** Oh, a bit of both. I mean, anytime I'm using AI, I'm telling it directions to go, what I want to focus on, and going through iterations to figure out what I think would be good. I don't know; I can't even remember. So yeah, I sometimes bounce back and forth in my use of AI on the blog. But definitely, if we're just trying to get a breakdown of the book, I think it's really helpful. Of course, you don't get my raw opinion about all of it, but...

**Participant 2:** I do\! Yeah, exactly. That's why I'm here. If I want *their* opinion, I can get that really easily. But the reason I'm coming to this live-ish session is because I want other people's takes on things.

**Tom:** Okay, well that will definitely be something we'll get into for sure. And I'll give you my raw opinion, and maybe that'll be a follow-up post. But before you can even have an intellectual discussion, right, we have to start with understanding the author's argument and all the different ideas going on here. So let's start there.

Well, actually, let's back up just a bit. General reactions to the book? Would you give this a thumbs up, a thumbs down, or somewhere in the middle? Did you like the book? Some people, I think, were posting that they kind of couldn't get into it, while others were really enthralled by it. Anybody want to share their general reaction to the book?

**Judith:** I loved it.

**Tom:** Okay. Any particular reason?

**Judith:** Because back in the 90s, when I lived in Texas, there was an organization consisting of academics and scholars who were discussing AI. And I've been watching AI for 30 years. So, I thought it was prescient and right on the money all the way through. I just loved it. I've already recommended it to two other people to read. So, I thought it was excellent.

**Tom:** Alright, Daniel, thanks.

**Daniel:** I also enjoyed it. I think I read the intro maybe a few weeks before I really jumped into it, and I was very ready to dislike it, but it grew on me. I particularly liked and appreciated Chapter 4, where it really gives an overview of recent progress with AI, especially deep learning and LLMs. He really kind of jumps into the argument of whether this is human intelligence, human thinking. And he says, basically, people say there's something inevitable about it; it's not, this is even better than human thinking. And I liked… it seemed like he used so many science fiction tropes in dramatizing all the things that can go wrong with AI. I appreciated that, even though in some ways that kind of catastrophizing is meant to bring attention and funding to it. But I like how he landed the plane with his policy suggestions. And I think he was also very aware of arguments against his own argument; he was very self-aware, which I appreciated as well.

**Tom:** Yeah, well, I share both of your sentiments about the book. Coming back to this earlier idea of my raw opinion: it's definitely a scary book. It makes me think, "Holy crap, massive changes are coming." My kids are just in college or going into college, and I wonder, are they even going to have a job? Does it even matter what they study? It'll be so massively different. I started to think, gosh, what will my role be in 10 years? Technical writing probably won't be around in the same way; maybe it'll be some crazy new hybrid, like some of the job titles I mentioned. So, I guess there was a certain fear. But then I also thought, "Wait a minute, don't we love to dwell on end-of-world scenarios?" The human mind seems drawn to these catastrophic apocalypses; it's what Hollywood bases every futuristic movie on. So maybe I'm just being pulled in this direction out of an innate psychological desire for technology to be this massive disruptor. But then, of course, in the book, he labels this as "pessimism aversion" and so on. Anyway, it's a book that prompts a lot of strong thoughts because it gets right at our livelihoods, our jobs, society, and stability. And yeah, it's kind of a scary thing. The main metaphor of the book is "the coming wave." Literally, if you think of a tsunami wave coming to decimate a city, it should spark some fear in the readers.

So, the second half of the book on containment really only makes sense if you first are persuaded and believe that there *is* a coming wave that will decimate society. Let's start there. Before we get into any sort of containment, do you believe that the future could take this very dark trajectory he has outlined? Do you really believe there's a coming wave, or do you feel it's overblown or exaggerated? Anybody have any thoughts on that part? Uh, yeah, go ahead, Karin.

**Karin:** Yes, I'm in France, so pardon my English. I'm actually one of the people who struggled with that book. I couldn't buy into this "end of the world" scenario from the start, so it was hard to keep reading it with a positive eye. I got the feeling that Suleyman, the founder of DeepMind, was saying, "What I built is so powerful and so good that it threatens the whole of humanity," which I thought was very presumptuous. It felt like he was just saying that AI is fantastic and you should buy it. So I read it more as a marketing speech for AI, and I didn't buy the idea that it was endangering the human species. Maybe that's me being too optimistic. But when I think that we human beings—well, *we*—have found agreements to stop the proliferation of nuclear weapons, I'm thinking, how can AI beat that?

**Tom:** I think that's a great point you brought up. I've heard that same argument in different spaces and podcasts: that the AI founders and companies are kind of overhyping AI's impact to justify the billions in investment coming in to fund the technology they need. Because it's going to be so massively impactful, it's going to rip apart society, or solve climate change, or bring miracles in biology and cure cancer, so of course, we need all this money. Maybe it *is* marketing hype. Interestingly, Suleyman doesn't counter that criticism, at least I don't remember it in the book. Maybe it's a more recent one. Sherry?

**Sherry:** Yeah, so he wrote this book probably in 2022-23, and just in the past two years, things have changed massively. I just finished reading the foreword he wrote for the paperback edition, where he said things changed—that was in September of '23. And things have changed even more since then. You know, the election has changed a lot of things. I'll just say that I was at a discussion recently about whether we're living in a post-truth society. And that, I think, is my biggest fear: it's going to become harder and harder to tell what is the truth.

**Tom:** Yeah, that's a good point about the book potentially being a little dated already. It's kind of crazy. I mean, by definition of his argument about the coming wave accelerating, coming faster, and transforming things, the book is going to be outdated faster than normal. Molly?

**Molly:** One thing I kept thinking about when I was reading the book was, I was trying to wonder who his target audience was. I was trying to imagine who he thought was going to read this book because, at times, I felt like he wasn't speaking to me. And that made me wonder, okay, so who is it? I'm curious what you all think: who is his audience?

**Tom:** That's a great question because I found myself feeling helpless. Like, yes, all these containment strategies seem okay, good. But what do *I* do? I'm just trying to keep my job, ride the wave, and not be unemployed in five years due to Devon or whoever replaces us as the AI, right? So, who... does anybody have any thoughts on who he is really talking to? Daniel?

**Daniel:** To me, it definitely feels like it's meant to be a more popular book. It's meant to... I mean, everyone uses AI or knows what it is. So, I definitely feel like in some ways it's a primer for what AI is, because even people who use it don't necessarily understand what it's doing. I feel like that's very much a part of why he chose to use science fiction tropes to dramatize his concerns. Because, you know, if you write a white paper, only a certain number of people are going to pay attention. But if you're talking about the end of the world and imagining all these really crazy scenarios in which it could end, that's going to drum up a lot of interest. And that's part of the reason the book was a New York Times bestseller.

**Tom:** Yeah, good point. But I mean, there are a lot of different formats a person could take. And for sure, he's adopting this more popular audience format—more general, more readable. It's not buried in footnotes or some other academic approach.

Alright, let's talk a little bit about... we were discussing whether we think the trajectories he's painting will really be big, decimating waves. It almost sounds silly to engage in this conversation, right? But at the same time, it's a significant part of the book. And hey, if you believe the world's going to end, how is it going to end? Somebody mentioned the 'info-pocalypse'—or however we're saying that term, 'information apocalypse'—where we can't tell true information from false. He outlined several different trajectories. This gets back to the 'fun with end-of-world scenarios,' but there was one with engineered pandemics, which probably hits pretty close to home and is obviously highly relevant if you're writing this in 2022\. Then there's synthetic biology, a field I'm less familiar with, which seems to involve more DNA editing with very inexpensive tools. It's easy to foresee pathogens being created, enhanced, or even just mistakes people make, and that getting out of hand.

Another scenario was misaligned AI—referring to AI goals at odds with human goals. This could even be inadvertent, where you set up some kind of defense... oh, actually, that's more the third one. But you just have AI working at odds with what humans want, leading to a loss of control.

The third one was autonomous warfare and escalation. Maybe you set up a system to respond immediately, but then a misfire triggers a cascade of actions that escalates war very fast.

We've mentioned the information apocalypse. Then there's economic collapse from workers being replaced, consumers not being able to buy anything, and all kinds of inequality.

Do any of these look more likely or more interesting to you? I thought I heard somebody raise their hand, but maybe I'm not seeing it anymore. Is it silly to even try to focus on the 'how'? Judith?

**Judith:** Well, one thing he didn't cover, though he kind of touched on it with autonomous warfare, is something I can see going absolutely haywire. I mean, the number of people in my neighborhood who have drones... if anybody got upset with someone else, they could... And these drones are not regulated; you don't know who owns them or whatever. I was on my back porch taking a nap one day, heard one, opened my eyes, and it was right over me, about 20 feet up. I was thinking, "What the heck?" If I'd had a rock, I'd have thrown it. But if something can come into your personal airspace and be armed, that really is a scary situation. He never touched on that exactly, but he kind of danced around it. He was talking more on a geopolitical level, I think. So, that's where my head went.

**Tom:** Yeah, the drones one is kind of scary too. We've had all kinds of drone news recently, and drones are playing a huge part in the Ukraine-Russia war and so on. Yes. Daniel?

**Daniel:** Yeah, in many ways, a lot of these scenarios overlap. To me, it feels like... one of the ways he's more traditionalist is that he feels the nation-state is the bedrock for security and has to be a real part of AI solutions. We are at a point where it's almost like the twilight of nation-states after 400 or 500 years. He also talked about the rise of multinational corporations, which in many ways have much more power and influence than nation-states (which have influence over their own region but also some within the world). To me, that intersection feels like the most serious, sort of midterm threat. The other one... he wrote very poignantly about the weavers in the afterword. In some ways, that's the kind of immediate thing we might be looking at: this skilled labor just getting moved into something totally different. But yeah, those are the kinds of things that I feel are catastrophes in their own right, I guess.

**Tom:** Yeah. Let me just comment on my thoughts on the nation-state thing. I think that is a very interesting intersection, as you point out. We're seeing right now this test of how powerful the government is versus not. Suleyman describes this dilemma of resorting to super-strong government lockdown to contain AI—by having something like ten times the surveillance state of China, where everything is monitored to really lock it down—in contrast to complete openness and chaos, where everything's open source, and anybody does whatever they want and creates all kinds of havoc. Trying to find the narrow path between those two alternatives is what most of his book tries to focus on.

But I find myself wondering, how powerful can these individual nation-states be in contrast to... say, all the tech companies banding together and using their AI to create their own little city communities or city-states with their own rules? At what point can you just push back and say, "Okay, government, you're not actually as powerful because we control a ton of power"? I don't know. This whole idea of fragmenting a nation into smaller subgroups is one he talks about. I think he mentioned groups like Hezbollah, who could be really empowered beyond their size and scope, having outsized influence due to being armed with AI or whatever tools they've got. Sherry?

**Sherry:** Yeah, so just to riff on that a little bit, I think he is talking about the fragility that we have experienced recently. We've had a lot of things kind of blow up that we expected, including possibly our own constitution. And this is something that AI... you know, we have no idea what the intersection of all that is going to be because we're dealing with things we have never dealt with on a scale we've never dealt with before. I suspect the author has read books like *The Black Swan* and *Antifragile*, where they talk about the fragility of the nation-state. I have to wonder whether there's just this intersection of all these things coming to a head all at once and how it's going to play out. So, I'm interested in hearing what other people think about that.

**Tom:** Yeah, the book is very timely in that regard, right? Because this was before the current political chaos and administration and so on, and he seemed to anticipate many of these themes. I find myself wondering: if AI is so powerful that people can engineer deepfakes and disinformation campaigns, why can't people harness this power to shape political outcomes against Trump, for example? Why is it that nobody can really counter any of the moves he seems to be making and all these executive orders and changes? It's like everybody's just sort of paralyzed. But I thought we were supposed to be empowered too. There's not even a bad actor acting for good, maybe. I don't know... I'm not really sure I want to steer into politics, but basically, yeah, it's one question I had: why can't people use AI to stop Trump? Judith?

**Judith:** Well, you can't pull it apart; it *is* part of it. I mean, just look at his inauguration, the people he had. In past presidential inaugurations, it's been the cabinet or people politically aligned with the person being inaugurated. He had the tech bros behind him. He had the owners of the largest technology companies in the country, in the world, sitting behind him. And it was... I was about to say something really crass... because it looked like a "mine is bigger than yours" kind of scenario to me. But anyway, I don't think you can pull it apart.

**Tom:** So, well, this display of power is certainly a major theme in the administration. And I think it fits in with Suleyman's trajectory around countries competing against each other. Can you imagine a scenario where we pull out of the AI race with China because we think we're worried about runaway AI scenarios or something? There's no way. There's no way the two countries would stop competing against each other. And even outside of country competition, just companies—we have all the big AI companies competing against each other at breakneck speed. It's not like we're going to see one company say, "Ah, you know what, I think we're going a little too fast. We're going to slow down. Our models are too powerful." It's not happening. The whole capitalistic structure and other structures are set up to just force acceleration and innovation that seems unchecked. So, yeah. Not really sure. Let's see, let's hear from somebody we haven't heard from. Superja?

**Superja:** I think the economic impact is the one that feels the most present for me right now, only because there seems to be some degree of impact already. When he talked about how AI is better at task completion, he specifically seemed to call out cognitive skills. And it seems like a lot of, I don't know, the way I think about our economy is based on completing some kind of task. So, maybe I don't know a lot about it, but that is one of my fears. It's like, the knowledge I have is no longer as valuable for task completion in that sense. Yeah, so I'm losing my words, but hopefully, someone can pick it up from there.

**Tom:** Yeah, that's a great point. Let's pick that up. But before we do, Sherry, you have one more thought you wanted to get out, right?

**Sherry:** Yeah, when you were talking about competition, I think there's this attitude that we are in competition *with* AI. Whereas, you know, there are certain things only humans can do, and certain things AI can do way better than us. But I think the bigger part of it is the things AI can help us do, that can supplement what humans can do. So, instead of being in competition, we should maybe think about how we can partner with AI, or let AI do the things it's good at and humans do the things we're good at, and we'll all be better together, instead of coming up with these doomsday scenarios. So that was my thought of optimism, so Molly doesn't have to go and curl up into a ball somewhere.

**Tom:** Yeah, I like this idea. Let's jump off on this point: trying to figure out where the human excels versus the machine. Superja said we're at a point where our knowledge and skills might not be valuable anymore; AI can complete certain tasks, and we no longer need humans for them. I've seen this, especially with lower-level tasks. Let's say you have a contractor come in to help out. Maybe before, you would give that contractor grunt work that involved a lot of processing, or "go and fix all these links in 5,000 pages." Now, you can basically have AI do that. AI is getting smarter and smarter. And yeah, it seems like engineers could probably just leverage AI to do many of the same tasks they previously gave to us. Personally, I think we need to focus on the really complex, complicated work—the stuff AI can't easily do. Because if we stick with this lower level of just editing and publishing what engineers produce, that's not going to take us very far. Daniel?

**Daniel:** Yeah, I wanted to jump off what Molly was saying. The labor question... I feel like he didn't go very deeply into it. He talked at the end about the Luddites destroying the weaving machines. And Suleyman acknowledged it was disastrous for them, but overall okay for everyone else. He talks about the division of labor, like in Ford companies, where highly trained craftsmen had their work divided up so anyone could do it. And it's not necessarily a very nurturing kind of work. I noticed the new job titles in your post; many of them are sort of maintenance or surveillance things, these specialized tasks. So we're at this point where writing, which is in some ways one of the most general kinds of knowledge production, is being sort of dismantled. And we are all kind of looking to find these specialties or these things we can hold on to in the aftermath, which we have to do.

**Tom:** Yeah, this definitely gets to the heart of the takeaway for me from the book: where do you focus if our skills in writing, editing, and publishing are becoming low value? How do we transition in a way that's still valuable? That's what this whole section I added on "potential skills" was hoping to accomplish. Like "AI-generated content authenticator"—this is actually a pretty good one. Not necessarily just someone who can identify deepfakes, but someone who can identify hallucinations or develop techniques to avoid mistakes, gaps, or errors in AI outputs. If you solve that, you can get any job you want. From the start, that's been the criticism of AI tools: they're sometimes wrong in subtle, hard-to-detect ways.

I have some strategies I use with docs to get around this, like uploading all the API reference into context sessions as a ground source of truth. But I feel we could take this to another level. How do you identify misinformation, or maybe not misinformation, but errors in generated info? And these other areas are almost like, "Hey, these could be areas to become much more familiar with." If these are the problems that could erupt, maybe it's an opportunity. The drone situation people brought up earlier—if you know a lot about drones, even documentation for them, or just robot defense, it seems like maybe that's a ripe area. It does seem kind of silly, again, to think about specializing in robot defense. It feels like I've swallowed a science fiction book and am suddenly mapping my career based on some far-reaching sci-fi stuff. But I don't know, what do you do? Let's see, somebody raised their hand. Sahare?

**Sahare:** Hello, sorry I cannot turn on my camera due to connectivity issues. I just wanted to add how we can spot hallucinations. My language is Urdu, and one day I tested ChatGPT and asked about a particular Urdu book that wasn't very popular and wasn't available in PDF format. Instead of saying the book was beyond its scope, ChatGPT started hallucinating and creating a story on its own. Another time, I had to write a review for a book that wasn't published yet, and I checked with ChatGPT, and it did the same thing. So, that's something... I mean, new knowledge that is coming in, AI cannot be trusted with it. Like new books that are coming out, AI isn't smart enough to be trusted with that kind of information. It's important to always keep in mind that whatever AI or ChatGPT or any other AI model is providing us is based on previously created information that humans provided. So there will always be a human element. Maybe in the future, our jobs will be more about overseeing what AI is doing, confirming if it's correct, and verifying the accuracy of the information. So I think humans will have jobs more related to that. But the hallucinations are always there. It's a very important and problematic issue that if AI doesn't know about a book, it still talks about it. I think that's a huge drawback of AI.

**Tom:** Yeah, I've got some thoughts on that. I've read research saying that experts are much more productive with AI tools than novices, for exactly the reason you described. As an expert, you can spot when things are wrong, and maybe you can correct the AI or just dismiss the incorrect parts. In the same way I used AI to get a summary of the book, I could understand if it was accurate or not, having read the book. If I hadn't read it, I'd just be shrugging, thinking, "Well, maybe it's accurate, maybe not. I have no idea." That kind of expertise, I think, will be critical to excelling with AI tools.

For example, a few months ago, I was trying to write code samples for a team using gRPC APIs, and I didn't have a strong sense of them, so I used AI to create them. I thought, "I'll just hand them to the engineers; they might make a few corrections, and then we'll publish them." While that worked for about three-quarters of these APIs, a few teams found all kinds of issues with the code. I was in this very vulnerable position of having to maybe put it back into the AI machine with their issues and hope the next version would fix them. I thought, "This is a bad spot for me to be in. I want to be the expert who's really doing the evaluation." So this need for expertise plays into our career strategy. We have to have expertise in something, and it's not going to be just about writing, right? If we're just generalists with great writing skills, maybe that doesn't take us where we need to be. We also have to add some subject matter domain expertise. I've got another example, but Sherry, you have thoughts?

**Sherry:** Yeah, just to add to the expertise area, there's also the issue of bias. I took an AI class on Coursera, and one example they had was: "Man is to king as woman is to queen." Pretty obvious. Then they put in: "Man is to software engineer as woman is to..." Anybody want to hazard a guess what the AI decided?

**Various Participants (guesses):** Secretary? QA Engineer? Technical writer?

**Sherry:** Housewife.

**Various Participants:** Housewife\! Oh my god\!

**Sherry:** Housewife, yes. I know. So it was kind of horrifying. This is just one very small example. You could see how if it was "farmer," something with a lot of historical literature, "farmer and housewife" might make some (albeit biased) sense. But this was "software engineer," a relatively recent profession, and yet it still came up with "housewife" as the equivalent for a woman. So, just a very small example of the bias. If you don't have people with expertise or people looking at the bias, we can go very badly wrong with some of these answers. Molly?

**Molly:** I think to add to that, Sherry, when I was reading this book, it was so interesting going from John Warner's book to this one. John Warner, right at the beginning, talked about all the problems with AI—bias being one, and also energy consumption. It was so interesting to come upon those same topics in this book and just be like, "Okay, where's the rebuttal going to happen?" Particularly with the bias one, I felt his rebuttal was kind of short, just like, "You know, we're taking care of it," and let's just dust it under the rug. It was an interesting experience. I really appreciated going from the previous book to this one and seeing how people rationalize things.

**Tom:** We could definitely pair more books as opposites to give us a better perspective. This next book on the list, *The Singularity is Near*, probably won't push back against AI, but we can throw some more critical stuff in there too. But yeah, for sure, awareness of bias is huge and probably should have been more developed in this book.

I had one more thought, and then I'll call on Robert and Karin. We were talking about expertise and the need to figure out where our expertise will be valuable. There's a neon sign basically blinking about where that currently is with writing: it's in developing AI-intelligent documentation systems. This is an area very few people understand how to do. I was just at the Write the Docs Portland conference, and one takeaway was that, yeah, everybody's just as confused about how to incorporate AI into documentation—for users, for authors, for analytics, everything. Nobody has a clear idea. There are different tools, experiences, and opinions, but it's pretty much all over the place. If you wanted to really get a job anywhere, become an expert in transforming documentation into something AI-powered. It sounds easy; it's really hard. How do you capture analytics? Create agentic workflows? Push through identified gaps? Identify uncertainty in responses? Evolve your tooling? Build your llms.txt files? Build an MCP server? All this stuff is like big question marks. But, at least for this little window of time—maybe two or three years, maybe two or three months, who knows?—that seems to be an area people could really exploit. Okay, let's go to Rob and then Karin.

**Rob:** Okay, got to turn on my mic. My response so far to the conversation, regarding concerns about losing jobs, is that I tend to think of AI as just another tool that's probably going to raise the bar in terms of quality. That's how I look at it. It's like when they introduced spreadsheets or PowerPoint; suddenly, you just had much better PowerPoint slides. So I think that's how I try to think of it. Maybe I'm being too optimistic, but for my sanity, that's how I approach it. And then, in terms of a practical method of overcoming problems with ChatGPT, I tend to throw my questions into multiple tools and then compare the results, and that works pretty well.

**Karin:** I pretty much agree with Rob in the sense that I don't feel threatened by AI as a technical writer. It's sort of an evolution of what we've all been doing all these years. I feel my value isn't just producing text, but making the right decisions on what to document, why, and how. Those things, I feel, cannot be taken over by an AI. And now I've got a great tool to do it, so I feel empowered by it, especially because I have to write in English, and it's not my native language. It's fantastic. I just have to focus on what needs to be written, how it works, ask the questions to the team, get my SMEs on board, and get the AI to spell it out clearly—which it obviously can do if you give the right instructions. You can even give it the style guide and everything, so it does a better job at it than I used to. It's a fantastic tool for me, especially writing in another language.

Also, one more thing about prompting: I found that giving a fallback plan to the model works pretty well. Saying something like, "If you cannot find any relevant information about this topic, then just say 'I don't know.'" If you don't say that, it really changes how the AI responds. If you just add that sentence, I found that, okay, it can eventually admit it doesn't find the answer, but you have to leave the door open for it. It's funny to do a bit of prompt engineering at our tech writing level.

**Tom:** Yeah, that sounds like a great tip. You've both touched upon another aspect: becoming familiar with AI tools and workflows and understanding how to avoid traps like hallucination by using the tip you just mentioned, Karin. And Rob mentioned using several tools to get a convergence of answers. That kind of awareness and fluency with AI tools is actually a non-obvious skill, I've found. Many tech writers just don't really know how to use the tools. And when they do, maybe they have a negative experience, it sours them, they become dismissive, skeptical, and then they fall farther behind because they never really learn the right approaches, prompts, and techniques to be successful. The idea that it's a non-obvious skill is still kind of mind-blowing. People don't automatically know how to use AI tools for the tasks they're trying to do. That is huge. Mary?

**Mary:** Hi. Just on what you were saying, I completely agree, it is a skill, and it's not obvious. So when it comes to putting the same material into multiple AIs, doesn't that take more time than just writing the thing anyway? I suppose, you know, I do use AIs, well I can, but most of the time I am generating new information. And Sahare, I think, said that AIs aren't generating what's not already there? And if they do, it's wrong.

**Tom:** Well, you make a good point that one could definitely spend too much time fiddling with AI when the task might be done more simply. It really depends on the task, I would say. But your other question is, okay, AI tools have a limited training body, so if you're writing about something new, how will it be relevant? This comes back to techniques. When I'm writing about a new feature, I find all the internal material I can. Usually, by the time it gets to me to document, there are product definition documents (PRDs), engineering design documents. Somebody's coded parts of an API, so there's a code artifact. You can find a lot of this stuff, gather it all up, stuff it into the machine, and use that as your input source. That is the golden technique.

In fact, the one that trumps all of those is reference content. When engineers have made a change in an API and you now have new source files, or you've generated reference docs with new elements described, you can just take that whole reference and put it in. Or you can capture the changelist, like the file diff of what's changed, and provide that to the AI. You can get around this problem of being trapped by the AI's limited training material. But just understanding the right techniques for the right situation really interests me because I love this feeling of empowerment. I love thinking, "God, these tools are so powerful. If I just knew how to use them in the right way to get what I want, then I extend my capabilities pretty far."

**Mary:** I see where you're at. But one of the things I like best about my job is that it's a communications role and I get to talk to people. I love it.

**Tom:** You actually talk to people\! Oh, so you go out and gather information manually, with people? Like you interview people about new features? You could still do that. You could do it even faster, in fact. Just record the meetings, have them draw on whiteboards too, whatever they want to do. I love the whiteboard. It's so efficient. Just take all those resources. That's actually one of the best techniques: if you can set up a meeting with someone and just get their brain to dump all the relevant information to you, even in a messy, unstructured, redundant format, feed that into your AI, and you're maybe 60-90% of the way there to whatever you need to write. You could shape it with, "Hey, match this pattern, use this style." Then you go through multiple iterations and say, "Ah, this paragraph doesn't feel right." Maybe you redo that yourself or push it through more iterations. I'm just saying the techniques for using AI are manifold and really just limited by your own creativity. Sherry?

**Sherry:** Yeah, to jump onto what you just said about creativity, that's one of the things people can do that AI might not be as good at, or it might be creative in a way we don't expect or even want. I loved what Mary said about actually talking to people. And then using AI if you record it and use the whiteboard. This is another thought about partnership: we do what we do really well. All the stuff you described, Tom, about how you do your job, that was how we did our job before AI. AI is just going to make it a little faster. So it's an enhancement, as opposed to a brand new way of doing things. And we probably haven't even thought of all the brand new ways we *can* do things because it's so new. And as you say, people don't know what the tools are or how to use them at this point. So, I think it's pretty exciting that there will be some new techniques. But for now, just enhancing the old techniques sounds like a pretty good plan.

**Mary:** So it seems to me, Tom, excuse me, you're saying that you can actually use AI to fill information gaps?

**Tom:** Uh, yeah, yeah. The social component Sherry touched upon is actually something I also have mixed feelings about. I've become more isolated because I feel I can just get the information myself from the changes engineers are making. I've been steered in the wrong direction many times by people who are misinformed about the product itself; I find out, "Oh yeah, it actually doesn't work that way or doesn't have that element." So I almost don't even want people to tell me. I want to rebuild the reference documentation, create a file diff, and discover for myself what's actually changed. Then, if there's stuff that's not apparent in that, fine, I'll go gather it manually. But I don't know. At the same time, that isolation isn't comforting, right? As you said, there's a communications role that's kind of nice, interacting with other people. I used to have monthly sync-ups with each team I supported, and we just stopped having topics, so I canceled all the meetings. I don't ever really need to meet with them. I sit near them, so I can chat if I need to, but there wasn't a need to sync more regularly because I felt I could just use AI to get all the information I needed. Maybe that's a delusion of mine. Anyway.

Okay, so let's talk a little more about this evolving role we have and how our job changes. What really should we be focusing on? Do you feel the need to move your documentation system, if you're a tech writer, into an AI-enabled era? Are you getting that kind of pressure? Or are you being pushed in another way? Maybe moving beyond a tech writer role, not just focusing on docs but something different, more project manager-y or QA related? I don't know, just how is your role evolving in this space? Karin?

**Karin:** Yeah, I do feel there's a change in what's expected from me, similar to when SEO came around. We weren't just writing for our audience anymore; we also had to write for search bots. And now, we also need to get into chat somehow. So, "Why is your content not picked up by the models?" is a question I have to find the answer to. It's about trying to find new ways of writing that LLMs like—more FAQs, maybe. Because we now assume people aren't only going to read or search the documentation or find it via Google, but they're also going to ask the AI about the product we're building. So we need to feed the LLMs now too. We have these multiple audiences, and the LLMs are one of my personas, somehow.

**Tom:** Yeah, I think you've hit upon what I believe is the most sought-after skill right now: the ability to get our documentation ingested into these models our users are using, so they can get accurate, complete information through these models, and get it fast. We've done research studies looking at how people use our docs, and often they're going to ChatGPT—literally ChatGPT, not even Gemini, even if they're using Google documentation sometimes. So how do you get all your information ingested? How do you make sure it's answering the questions people are asking? Those are the skills really in demand. I mentioned llms.txt files; if you haven't heard of those, that's one potential mechanism, like a sitemap for large language models to ingest. I'm not even sure if it works, but it's one thing some people are pushing. There are probably so many other opportunities. But just identifying that problem space and figuring out how to move into it and become experts would be great. Imagine, no matter what domain becomes hot—cybersecurity, information corroboration, whatever—the ability to create an AI-powered documentation system that delivers accurate answers will be required for every subject matter domain. That would be very good to become an expert in.

So, alright, we are running out of time. I just wanted to mention the next book in the list for people. Oh, not sharing my screen, let me share real quick. I did mention it already: it's the Singularity book. I haven't read it yet, but it got good reviews, so I thought it would be interesting. It's Ray Kurzweil. I guess he had an initial book, *The Singularity is Near*, and now he has a follow-up. On Amazon, it has over a thousand reviews, four and a half stars, so hopefully, it's good. It's been a New York Times bestseller, so we'll see. It's definitely getting more into merging with AI, and hopefully, we can look at how to augment our human skills with AI to tackle these complex, wicked projects or whatever kinds of things we can't do with our regular capabilities. Anyway, that meeting is set for, let's see, June 15th. So, thanks again for coming to this. I really appreciate your thoughts and participation. I love your insights and love the group here. Thanks again, and I'll post a recording very soon next week for those who couldn't make it. Alright, have a good rest of your weekend.

**All:** Thank you\! Thanks, Tom\! Bye-bye\!